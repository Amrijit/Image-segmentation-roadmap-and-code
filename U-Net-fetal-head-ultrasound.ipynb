{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5374586,"sourceType":"datasetVersion","datasetId":3118209}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %cd /content/sample_data\n%cd /kaggle/working/\n\n!mkdir training_images\n!mkdir annotated_training_images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport os\n\n# %cd /content/sample_data/training_set\n# Here, the path points to a dataset of fetal head ultrasound images located in the Kaggle environment.\n%cd /kaggle/input/fetal-head-ultrasound-dataset-for-image-segment/training_set/training_set\nfor filename in os.listdir():\n    if filename.endswith('.jpg') or filename.endswith('.png'):\n        if 'Annotation' in filename:\n            shutil.copy(filename, '/kaggle/working/annotated_training_images')\n        else:\n            shutil.copy(filename, '/kaggle/working/training_images')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The code reads a grayscale image (828_HC_Annotation.png) that is likely a mask or annotation image.\n# It finds the contours (boundary points) of shapes in this image using cv2.findContours().\n# Then, it draws the first detected contour onto the image, filling the contour area with white (255).\n\nimport cv2\nmask1 = cv2.imread(\"/kaggle/working/annotated_training_images/828_HC_Annotation.png\",0)\n\nmask = cv2.imread(\"/kaggle/working/annotated_training_images/828_HC_Annotation.png\",0)  \nmask_contour= cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)       #  detects boundaries\nmask_img = cv2.drawContours(mask,mask_contour[0],0,255, -1)      # draws boundary on an image\n\n\nimg = cv2.cvtColor(mask1, cv2.COLOR_BGR2RGB)\nimport matplotlib.pyplot as plt\n# Show the image\nplt.imshow(img)\nplt.show()\n\nimg1 = cv2.cvtColor(mask_img, cv2.COLOR_BGR2RGB)\nplt.imshow(img1)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nfrom torchvision.io import read_image\nimport torch\nimport matplotlib.pyplot as plt\n\ndef pad_image_to_square(img):              # make the image 812x812\n\n\n  channels, height, width = img.size()\n  diff = abs(812-width)                # amount of pixels that need to be added to make the image’s width 812\n \n  padding = diff // 2    # Determine the amount of padding to add on each side\n  \n  # to make it 812*812\n  img = torch.nn.functional.pad(img, (padding, diff - padding, 0, 0), mode='constant', value=0)\n  # print(img.shape)\n  \n  # print(\"hey\" , img.size())\n\n  channels, height, width = img.size()\n  diff = abs(height-width)             #  amount of pixels that need to be added to make the image’s height 812\n \n  padding = diff // 2   # Determine the amount of padding to add on each side\n\n\n  # Pad the tensor with zeros along the height and width dimensions\n# height is smaller than the width, padding is added to the top and bottom else added to left right\n  if height < width:\n      padded_image = torch.nn.functional.pad(img, (0, 0, padding, diff - padding), mode='constant', value=0)\n  else:\n      padded_image = torch.nn.functional.pad(img, (padding, diff - padding, 0, 0), mode='constant', value=0)\n\n\n  return padded_image\n\n\n# Open the rectangular image\nimg = read_image('/kaggle/working/training_images/100_2HC.png')\n\n\n# Pad the image to make it square\npadded_img = pad_image_to_square(img)\nprint(padded_img.shape)\n\ntensor_image = padded_img.permute(1, 2, 0)\nplt.imshow(tensor_image)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport cv2\nimport os\nimport pandas as pd\nfrom torchvision.io import read_image\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, img_path, target_path, transform , target_transform):\n        self.img_path = img_path\n        self.target_path = target_path\n        self.transform = transform\n        self.target_transform = target_transform\n\n\n    def __len__(self):\n        return len(os.listdir(self.img_path))\n\n    def __getitem__(self, index):\n         # The code generates the full file path to the image located at the specified index in the sorted list of files\n        img_path = os.path.join(self.img_path, sorted(os.listdir(self.img_path))[index])\n        img = read_image(img_path)   \n        img = pad_image_to_square(img)\n\n        if self.target_path:\n          img_path_tar = os.path.join(self.target_path, sorted(os.listdir(self.target_path))[index])\n\n          target = cv2.imread(img_path_tar , 0)\n          mask_contour= cv2.findContours(target, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n          cv2.drawContours(target,mask_contour[0],0,255, -1)\n          target = target[np.newaxis, :, :]                   # add a new axis (1, height, width)\n          target = torch.from_numpy(target)\n          target = pad_image_to_square(target)\n        else :\n          target = torch.tensor([])\n          \n\n        if self.transform:\n            img = self.transform(img)\n        if self.target_transform:\n            # print(\"target\", target.shape, type(target))\n            target = self.target_transform(target)\n            target = torch.where(target > 128, torch.tensor([1.0]), torch.tensor([0.0]))  # This binarizes the target tensor\n\n        return img.float(), target.float()\n\n\n\n# Define the transformation to resize the images to 64x64 pixels\ntransform = transforms.Compose([\n    transforms.Resize((252, 252))   #original size in paper 572 , 572\n    # transforms.ToTensor()\n])\n\ntransform1 = transforms.Compose([\n    transforms.Resize((68, 68))   # original size in paper 388 , 388'\n\n])\n# Instantiate dataset object\ndataset = CustomDataset(\n\n    img_path = '/kaggle/working/training_images' ,\n    target_path = '/kaggle/working/annotated_training_images' ,\n    transform=transform , \n    target_transform = transform1 )\n\ntest_dataset = CustomDataset(\n    img_path = '/kaggle/input/fetal-head-ultrasound-dataset-for-image-segment/test_set/test_set' ,\n    target_path = None ,\n    transform=transform , \n    target_transform = None\n    )\n\n# justto check number of images\nimg_path = '/kaggle/working/training_images'\nprint(len(os.listdir(img_path)))\ntarget_path = '/kaggle/working/annotated_training_images'\nprint(len(os.listdir(target_path)))\n\n\nimport torch.utils.data as data\ntrain_dataset, val_dataset = data.random_split(dataset, [799, 200])\n\n\n# Create train and validation dataloaders\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True) #,worker_init_fn=lambda _: random.seed(42))\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False) #,worker_init_fn=lambda _: random.seed(42))\n\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False) #,worker_init_fn=lambda _: random.seed(42))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\ni = 0\nimport itertools\nfor i, batch in enumerate(train_dataloader):\n  if i == 1:\n      break\n\n  # Plot each image in a separate subplot\n  fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(15, 5))\n  for i in range(5):\n      axs[i].imshow(batch[0][i, 0], cmap='gray')\n      axs[i].axis('off')\n  plt.show()\n  i+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\ni = 0\nimport itertools\nfor i, batch in enumerate(train_dataloader):\n  if i == 1:\n      break\n  # print(f\"Batch {i+1}: {batch}\")\n\n  # Plot each image in a separate subplot\n  fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(15, 5))\n  for i in range(5):\n      axs[i].imshow(batch[1][i, 0], cmap='gray')\n      axs[i].axis('off')\n  plt.show()\n  i+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef scale_tensor(image ,new_height , new_width):\n  scaled_image = F.interpolate(image, size=(new_height, new_width), mode='bilinear', align_corners=True) # resize tensors\n  return scaled_image\n\n\n#double 3x3 convolution \ndef dual_conv(in_channel, out_channel):\n    conv = nn.Sequential(\n        nn.BatchNorm2d(in_channel),\n        nn.Conv2d(in_channel, out_channel, kernel_size=3),\n        nn.BatchNorm2d(out_channel),\n        nn.ReLU(inplace= True),\n        nn.Conv2d(out_channel, out_channel, kernel_size=3),\n        nn.BatchNorm2d(out_channel),\n        nn.ReLU(inplace= True),\n    )\n    return conv\n\n\n# This function crops the input tensor so that its spatial dimensions match the dimensions of the target_tensor\n# as shown in architecture image , half left side image is concated with right side image (skip connections !)\ndef crop_tensor(target_tensor, tensor):\n    target_size = target_tensor.size()[2]\n    tensor_size = tensor.size()[2]\n    delta = tensor_size - target_size\n    delta = delta // 2\n\n    return tensor[:, :, delta:tensor_size- delta, delta:tensor_size-delta]\n\ndef out_layer(in_channel , out_channel):\n  conv_layer = nn.Sequential(\n      nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=1),\n      nn.BatchNorm2d(out_channel),\n      nn.Sigmoid()\n  )\n  return conv_layer\n\nclass Unet(nn.Module):\n    def __init__(self):\n        super(Unet, self).__init__()\n\n        # Left side (contracting path)\n        self.dwn_conv1 = dual_conv(1, 64)\n        self.dwn_conv2 = dual_conv(64, 128)\n        self.dwn_conv3 = dual_conv(128, 256)\n        self.dwn_conv4 = dual_conv(256, 512)\n        self.dwn_conv5 = dual_conv(512, 1024)\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        #Right side  (expnsion path) \n        #transpose convolution is used showna as green arrow in architecture image\n        self.conv_bn=nn.BatchNorm2d(1024)\n\n        self.trans1 = nn.ConvTranspose2d(1024,512, kernel_size=2, stride= 2)\n        self.up_conv1 = dual_conv(1024,512)\n        self.trans2 = nn.ConvTranspose2d(512,256, kernel_size=2, stride= 2)\n        self.up_conv2 = dual_conv(512,256)\n        self.trans3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride= 2)\n        self.up_conv3 = dual_conv(256,128)\n        self.trans4 = nn.ConvTranspose2d(128,64, kernel_size=2, stride= 2)\n        self.up_conv4 = dual_conv(128,64)\n\n        #output layer\n        # self.out = nn.Conv2d(64, 1, kernel_size=1) \n        self.out = out_layer(64 , 1)\n\n    def forward(self, image):\n\n        #Padding the image to make it sqare\n        # image = pad_image_to_square(image)\n        #forward pass for Left side\n        \n        x1 = self.dwn_conv1(image)\n        x2 = self.maxpool(x1)\n        \n        x3 = self.dwn_conv2(x2)\n        x4 = self.maxpool(x3)\n        \n        x5 = self.dwn_conv3(x4)\n        x6 = self.maxpool(x5)\n        \n        x7 = self.dwn_conv4(x6)\n        x8 = self.maxpool(x7)\n        \n        x9 = self.dwn_conv5(x8)\n        \n\n        #forward pass for Right side\n        x_norm = self.conv_bn(x9)\n        \n        x = self.trans1(x_norm)\n        y = crop_tensor(x, x7)\n        x = self.up_conv1(torch.cat([x,y], 1))\n\n        x = self.trans2(x)\n        y = crop_tensor(x, x5)\n        x = self.up_conv2(torch.cat([x,y], 1))\n\n        x = self.trans3(x)\n        y = crop_tensor(x, x3)\n        x = self.up_conv3(torch.cat([x,y], 1))\n\n        x = self.trans4(x)\n        y = crop_tensor(x, x1)\n        x = self.up_conv4(torch.cat([x,y], 1))\n        \n        x = self.out(x)\n\n        \n        return x\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n# from dataset import MyDataset\n# from model import UNet\n\ntorch.cuda.empty_cache()\n\nimport gc\n# del inputs , targets , model\ngc.collect()\n\n\n\n# Define hyperparameters\n# batch_size = 2\nlearning_rate = 0.001\nnum_epochs = 10\n\n\n\n# Define the U-Net architecture\nmodel = Unet()\nif torch.cuda.is_available():\n  model.cuda()\n\n# Define the loss function and optimizer\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nv_loss , train_loss = [] , []\n# Train the U-Net\nfor epoch in range(num_epochs):\n  model.train()\n  for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n      # Forward pass\n      inputs = inputs.to(device)   #\n      targets = targets.to(device)#\n\n      outputs = model(inputs)\n      loss = criterion(outputs, targets)\n\n      # Backward pass\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n\n  # Evaluate the U-Net on the validation set\n  with torch.no_grad():\n    model.eval()\n    val_loss = 0\n    for inputs, targets in val_dataloader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        outputs = model(inputs)\n        val_loss += criterion(outputs, targets)\n    val_loss /= len(val_dataloader)\n  v_loss.append(val_loss.item())\n  train_loss.append(loss.item())\n  # Print the loss for the current epoch\n  print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}  , Val Loss: {val_loss.item():.4f}\")\n\n# Save the trained model\n# torch.save(model.state_dict(), 'path/to/trained/model')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dice_coefficient(y_pred, y_true):\n    smooth = 0.001\n    y_pred = y_pred.view(-1)\n    # print(type(y_pred), y_pred.shape)\n    y_true = y_true.view(-1)\n    intersection = torch.sum(y_pred * y_true)\n    dice = (2.0 * intersection + smooth) / (torch.sum(y_pred) + torch.sum(y_true) + smooth)\n    return dice\n\ndef dice_score(y_pred_batch, y_true_batch):\n    dice_scores = torch.zeros(y_pred_batch.shape[0])\n    for i in range(y_pred_batch.shape[0]):\n        dice_scores[i] = dice_coefficient(y_pred_batch[i], y_true_batch[i])\n    return dice_scores\n\nwith torch.no_grad():\n  dice_com = torch.tensor([])\n  model.to(device)\n  model.eval()\n  val_loss = 0\n  for inputs, targets in val_dataloader:\n      inputs = inputs.to(device)\n      targets = targets.to(device)#\n\n      outputs = model(inputs)\n      outputs = torch.where(outputs > 0.5, torch.tensor([1.0]).to(device), torch.tensor([0.0]).to(device))\n      dice = dice_score(outputs, targets)\n      dice_com = torch.cat((dice_com, dice))\n\nprint(\"Dice Score of Validation Set\")\ndice_com.mean().item()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(v_loss)\nplt.plot(train_loss)\nplt.legend([\"Validation loss\" , \"Training loss\"])\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Save the trained model\n# torch.save(model.state_dict(), '/content/drive/MyDrive/ML-Assign-6/model1.pt')\n# # model.load_state_dict(torch.load(model_path))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = Unet()\n# if torch.cuda.is_available():\n#   model.cuda()\n# model.load_state_dict(torch.load('/content/drive/MyDrive/ML-Assign-6/model.pt'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n# del model \nimport gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\n# Get the first 2 batches\ntop_2_batches = list(itertools.islice(test_dataloader, 2))\n\nout = []\ntest = []\ntest1 = []\n# Process the batches\nwith torch.no_grad():\n  for batch in top_2_batches:\n      # Do something with the batch\n\n      inputs = batch[0].to(device)   #\n\n      test.append(inputs)\n      test1.append(batch[1].to(device))\n\n      outputs = model(inputs)\n      out.append(outputs)\n\n      torch.cuda.empty_cache()\n      gc.collect()\n      print(batch[0].shape)\n  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\n\n# Create a tensor of size [5, 1, 628, 628] (replace with your own tensor)\ntensor = out[0].cpu().detach().numpy()\n\n# Plot each image in a separate subplot\nfig, axs = plt.subplots(nrows=1, ncols=5, figsize=(15, 5))\nfor i in range(5):\n    axs[i].imshow(tensor[i, 0], cmap='gray')\n    axs[i].axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\n\n# Create a tensor of size [5, 1, 628, 628] (replace with your own tensor)\ntensor_train = test[0].cpu().detach().numpy()\n\n# Plot each image in a separate subplot\nfig, axs = plt.subplots(nrows=1, ncols=5, figsize=(15, 5))\nfor i in range(5):\n    axs[i].imshow(tensor_train[i, 0], cmap='gray')\n    axs[i].axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}